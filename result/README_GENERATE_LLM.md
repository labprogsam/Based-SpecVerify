# Guia de Uso: GeraÃ§Ã£o de CÃ³digo com LLMs Diferentes por Etapa

Este guia explica como usar o script `generate_llm_code.py` para gerar especificaÃ§Ãµes formais e cÃ³digo de verificaÃ§Ã£o usando LLMs diferentes em cada etapa do processo.

## ğŸ“‹ VisÃ£o Geral

O processo de verificaÃ§Ã£o tem duas etapas principais:

1. **Etapa 1 - Formalized Req & Test Design**: Gera `formal_specification.txt` analisando o cÃ³digo C e requisitos
2. **Etapa 2 - Property Verification**: Gera `ert_main.c` com asserÃ§Ãµes baseado na especificaÃ§Ã£o formal

Agora vocÃª pode usar **LLMs diferentes** em cada etapa!

## ğŸš€ InstalaÃ§Ã£o

1. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

2. Configure as variÃ¡veis de ambiente com suas API keys:
```bash
# Para OpenAI (ChatGPT)
export OPENAI_API_KEY="sua-chave-aqui"

# Para Anthropic (Claude)
export ANTHROPIC_API_KEY="sua-chave-aqui"
```

Para Windows PowerShell:
```powershell
$env:OPENAI_API_KEY="sua-chave-aqui"
$env:ANTHROPIC_API_KEY="sua-chave-aqui"
```

## ğŸ“– Uso BÃ¡sico

### MÃ©todo 1: Usando argumentos da linha de comando

```bash
python generate_llm_code.py --task 1_fsm --phase1 ChatGPT --phase2 Claude
```

Isso irÃ¡:
- Usar **ChatGPT** para gerar a especificaÃ§Ã£o formal (Etapa 1)
- Usar **Claude** para gerar o cÃ³digo de verificaÃ§Ã£o (Etapa 2)
- Salvar os resultados em `result/1_fsm/LLM_code/ChatGPT_Claude/`

### MÃ©todo 2: Usando arquivo de configuraÃ§Ã£o

1. Edite `llm_config.json` para adicionar suas combinaÃ§Ãµes preferidas
2. Execute:
```bash
python generate_llm_code.py --task 1_fsm --config llm_config.json --combination ChatGPT_Claude
```

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Arquivo de ConfiguraÃ§Ã£o (llm_config.json)

O arquivo `llm_config.json` permite definir combinaÃ§Ãµes prÃ©-configuradas:

```json
{
  "combinations": [
    {
      "name": "ChatGPT_Claude",
      "phase1": {
        "name": "ChatGPT",
        "provider": "openai",
        "model": "gpt-5",
        "api_key_env": "OPENAI_API_KEY"
      },
      "phase2": {
        "name": "Claude",
        "provider": "anthropic",
        "model": "claude-3-5-sonnet-20241022",
        "api_key_env": "ANTHROPIC_API_KEY"
      }
    }
  ]
}
```
##### versoes de llms
claude: https://platform.claude.com/docs/en/about-claude/models/overview
chatgpt: GET - https://api.openai.com/v1/models (AutenticaÃ§Ã£o de Bearer token sendo ele: Bearer api_key)

### Usando Ollama (Llama local)

Para usar modelos Llama executados localmente via Ollama:

1. Instale e inicie o Ollama:
```bash
# Instalar Ollama (veja https://ollama.ai)
ollama serve

# Em outro terminal, baixe um modelo:
ollama pull llama3.1
```

2. Use a combinaÃ§Ã£o `Llama_Llama` ou crie uma nova:
```bash
python generate_llm_code.py --task 1_fsm --phase1 Llama --phase2 Llama
```

### API Keys em Arquivo (Opcional)

VocÃª pode criar um arquivo `api_keys.json` (nÃ£o commitado no git):

```json
{
  "OPENAI_API_KEY": "sua-chave-openai",
  "ANTHROPIC_API_KEY": "sua-chave-anthropic"
}
```

E usar:
```bash
python generate_llm_code.py --task 1_fsm --phase1 ChatGPT --phase2 Claude --api-keys api_keys.json
```

## ğŸ“ Estrutura de DiretÃ³rios

ApÃ³s a geraÃ§Ã£o, a estrutura serÃ¡:

```
result/
  â””â”€â”€ 1_fsm/
      â””â”€â”€ LLM_code/
          â”œâ”€â”€ ChatGPT_Claude/          # Nova estrutura: combinaÃ§Ã£o de LLMs
          â”‚   â”œâ”€â”€ formal_specification.txt
          â”‚   â”œâ”€â”€ ert_main.c
          â”‚   â”œâ”€â”€ fsm_12B_global.c
          â”‚   â””â”€â”€ fsm_12B_global.h
          â”œâ”€â”€ Claude_ChatGPT/          # Outra combinaÃ§Ã£o
          â”œâ”€â”€ ChatGPT_code/            # Estrutura antiga (compatÃ­vel)
          â””â”€â”€ Claude_code/             # Estrutura antiga (compatÃ­vel)
```

## âœ… VerificaÃ§Ã£o

ApÃ³s gerar o cÃ³digo, execute a verificaÃ§Ã£o:

```bash
cd result
python run_verification.py
```

O script `run_verification.py` foi atualizado para suportar tanto a estrutura antiga quanto a nova.

## ğŸ¯ Exemplos de Uso

### Exemplo 1: ChatGPT â†’ Claude
```bash
python generate_llm_code.py --task 1_fsm --phase1 ChatGPT --phase2 Claude
```

### Exemplo 2: Claude â†’ ChatGPT
```bash
python generate_llm_code.py --task 1_fsm --phase1 Claude --phase2 ChatGPT
```

### Exemplo 3: Usando configuraÃ§Ã£o
```bash
python generate_llm_code.py --task 0_triplex --config llm_config.json --combination Claude_ChatGPT
```

### Exemplo 4: Llama local
```bash
python generate_llm_code.py --task 1_fsm --phase1 Llama --phase2 Llama
```

## ğŸ” Troubleshooting

### Erro: "API key nÃ£o encontrada"
- Verifique se as variÃ¡veis de ambiente estÃ£o configuradas
- Ou forneÃ§a um arquivo `api_keys.json` com `--api-keys`

### Erro: "Prompt nÃ£o encontrado"
- Certifique-se de que os arquivos `prompt1_*.txt` e `prompt2_*.txt` existem em `result/`
- O script tentarÃ¡ usar um fallback (prompt1_Claude.txt)

### Erro: "Biblioteca nÃ£o instalada"
- Execute: `pip install -r requirements.txt`
- Para Ollama, certifique-se de que o servidor estÃ¡ rodando

### Erro: "Arquivo de cÃ³digo C nÃ£o encontrado"
- Verifique se o diretÃ³rio da tarefa existe em `result/`
- O script procura arquivos `.c` em subdiretÃ³rios de `LLM_code/`

## ğŸ“ Notas

- Os prompts (`prompt1_*.txt` e `prompt2_*.txt`) sÃ£o especÃ­ficos para cada LLM
- O script copia automaticamente os arquivos `.c` e `.h` necessÃ¡rios para o diretÃ³rio de saÃ­da
- A estrutura antiga (`ChatGPT_code`, `Claude_code`) continua funcionando normalmente
- O `run_verification.py` suporta ambas as estruturas automaticamente

## ğŸ”„ Compatibilidade

O cÃ³digo gerado Ã© compatÃ­vel com o script de verificaÃ§Ã£o existente (`run_verification.py`). VocÃª pode:

1. Gerar cÃ³digo com diferentes combinaÃ§Ãµes de LLMs
2. Executar verificaÃ§Ã£o normalmente
3. Comparar resultados entre diferentes combinaÃ§Ãµes

## ğŸ“Š Comparando Resultados

Para comparar resultados de diferentes combinaÃ§Ãµes:

```bash
cd result
python run_verification.py
```

O arquivo `verification_summary.csv` conterÃ¡ resultados de todas as combinaÃ§Ãµes encontradas.

